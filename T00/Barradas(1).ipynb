{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "WUmecs2bXdww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load pacakages\n",
        "For our exercises, you should always use the standar packages from Google Colab unless stated otherwise."
      ]
    },
    {
      "metadata": {
        "id": "Mr3-yF_1XGO7",
        "colab_type": "code",
        "outputId": "9cb41294-ca70-4751-c44f-8e01cba725b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "print(tf.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "1.16.3\n",
            "0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1bD2az2VXwtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data function\n",
        "Here you must write down your data loading function"
      ]
    },
    {
      "metadata": {
        "id": "a8w13xdzXtNe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loadTrainData (default_dir=\"/default/path/if/needed\"):\n",
        "  x = np.random.rand(16000,1) # write our data loading procedures\n",
        "  y = np.sin(x)\n",
        "  return x , y\n",
        "\n",
        "def loadTestData (default_dir=\"/default/path/if/needed\"):\n",
        "  x = np.random.rand(160,1) # write our data loading procedures\n",
        "  y = np.sin(x)\n",
        "  return x , y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CYjifVRqbLMr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Structure your model\n",
        "Create a class to define and use your model"
      ]
    },
    {
      "metadata": {
        "id": "y0Fv2md1YyKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  \"\"\"\n",
        "  Tensor Flow fully connected network model. \n",
        "  `Model(learning_rate=0.0001, mini_batches=16, neurons=[4])`\n",
        "  Parameters\n",
        "  \n",
        "  learning_rate : the learning rate parameter for the optimization method\n",
        "  mini_batches : Number of elements in every batch for the learning phase\n",
        "  layers : list of integers representing the number of neurons on every layer\n",
        "  \"\"\"\n",
        "  def __init__(self,learning_rate=0.0001, mini_batches=16, layers=[4]):\n",
        "    self.lr = learning_rate\n",
        "    self.mb = mini_batches\n",
        "    \n",
        "    # Input and labels can only be float32\n",
        "    self.input = tf.placeholder(tf.float32, shape=(None, 1), name=\"X\")\n",
        "    self.label = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n",
        "    \n",
        "    # Initialization of model\n",
        "    self.output = self.input\n",
        "    for neurons in layers:\n",
        "      self.output = tf.layers.dense(self.output, neurons)\n",
        "      \n",
        "    self.output = tf.layers.dense(self.output, 1) # last dense layer\n",
        "    \n",
        "    # Definition of the loss\n",
        "    self.loss = (self.output - self.label) * (self.output - self.label)\n",
        "    self.loss = tf.reduce_mean(self.loss)\n",
        "    \n",
        "  def get_loss(self):\n",
        "    \"\"\"loss getter method\"\"\"\n",
        "    return self.loss\n",
        "  \n",
        "  def get_xy(self):\n",
        "    \"\"\"input and label getter\"\"\"\n",
        "    return self.input, self.label\n",
        "  \n",
        "  def get_output(self):\n",
        "    \"\"\"output getter\"\"\"\n",
        "    return self.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG8wbG2IbT2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create an optimization routine\n",
        "Define your optimization steps"
      ]
    },
    {
      "metadata": {
        "id": "KaVxN-qMaY9v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "  \"\"\"\n",
        "  Learning algorithm for a TensorFlow model\n",
        "  `Optimizer(model)`\n",
        "  model : a fully connected TensorFlow neural network\n",
        "  \"\"\"\n",
        "  def __init__(self,model):\n",
        "    self.model     = model\n",
        "    self.loss      = model.get_loss()\n",
        "    self.X, self.Y = model.get_xy()\n",
        "    self.mb        = model.mb # Mini batches\n",
        "    self.output    = model.get_output()\n",
        "    \n",
        "    # Using GD as optimizier algorithm\n",
        "    self.opt       = tf.train.GradientDescentOptimizer(learning_rate = model.lr)\n",
        "    self.optAction = self.opt.minimize(self.loss) # minimize loss\n",
        "    \n",
        "  def batching (self, size):\n",
        "    \"\"\"Return the indexes of the input to be used for minibatch\"\"\"\n",
        "    r=np.arange(int(size))\n",
        "    np.random.shuffle(r)\n",
        "    return r\n",
        "  \n",
        "  def train (self, dataX, dataY, verbose):\n",
        "    \"\"\"Training of the NN\"\"\"\n",
        "    i     = 0 # A counter for the number of data processed\n",
        "    loss  = 0 \n",
        "    count = 0 # Another counter, for the number of minibatches required\n",
        "    batchOrder = self.batching(len(dataX)) # Get indexes of batch data\n",
        "#     print (batchOrder)\n",
        "    while (i+self.mb <= len(dataX)): # for all elements in the input \n",
        "      \n",
        "      # get the batch data\n",
        "      mbX, mbY   = dataX[batchOrder[i:i+self.mb]] , dataY[batchOrder[i:i+self.mb]]\n",
        "      \n",
        "      # run optimization\n",
        "      _ , mbLoss = self.sess.run([self.optAction, self.loss],\n",
        "                                   feed_dict={\n",
        "                                       self.X:mbX,\n",
        "                                       self.Y:mbY\n",
        "                                   })\n",
        "      if verbose>1:\n",
        "        print(\"\\t Inner loss: \"+str(mbLoss))\n",
        "          \n",
        "      loss  += mbLoss\n",
        "      i     += self.mb\n",
        "      count += 1\n",
        "    loss = loss / count\n",
        "    return loss\n",
        "  \n",
        "  def test  (self, dataX, dataY, verbose):\n",
        "    i     = 0 # A counter for the number of data processed\n",
        "    loss  = 0\n",
        "    count = 0 # Another counter, for the number of minibatches required\n",
        "    batchOrder = self.batching(len(dataX)) # Get indexes of batch data\n",
        "    \n",
        "    while (i+self.mb <= len(dataX)): # for all elements in the input \n",
        "      \n",
        "      # get test data for the batch  \n",
        "      mbX, mbY   = dataX[i:i+self.mb] , dataY[i:i+self.mb]\n",
        "      \n",
        "      # get the loss on the test set\n",
        "      mbLoss = self.sess.run(self.loss,\n",
        "                                   feed_dict={\n",
        "                                       self.X:mbX,\n",
        "                                       self.Y:mbY\n",
        "                                   })          \n",
        "      loss  += mbLoss\n",
        "      i     += self.mb\n",
        "      count += 1\n",
        "    loss = loss / count\n",
        "    return loss\n",
        "  \n",
        "  def run   (self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
        "    \"\"\"Run the learning phase and return the test and train history\"\"\"\n",
        "    historyTR = []\n",
        "    historyTS = []\n",
        "    with tf.Session() as self.sess:\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      lossTS = self.test  (testX , testY, verbose)\n",
        "      historyTR.append(lossTS)\n",
        "      historyTS.append(lossTS)\n",
        "      for i in range(epochs):\n",
        "        \n",
        "        lossTR = self.train (dataX , dataY, verbose)\n",
        "        lossTS = self.test  (testX , testY, verbose)\n",
        "        if verbose > 0:\n",
        "          print(\"Epoch \" +str(i+1)+\" : Train Loss = \" + str(lossTR)+\" :  Test Loss = \" + str(lossTS))\n",
        "        historyTR.append(lossTR)\n",
        "        historyTS.append(lossTS)\n",
        "    return historyTR, historyTS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLQQTXhZb2N4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Structure your calls\n",
        "Here you should make the main calls"
      ]
    },
    {
      "metadata": {
        "id": "gH38m2ykb18C",
        "colab_type": "code",
        "outputId": "e585f090-4967-4d35-e20d-80e4d902d84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1858
        }
      },
      "cell_type": "code",
      "source": [
        "x  , y  = loadTrainData ()\n",
        "xt , yt = loadTestData  ()\n",
        "\n",
        "model  = Model ()\n",
        "opt    = Optimizer (model)\n",
        "tr, ts = opt.run (x, y, xt, yt, 100, verbose=1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-5837a2109dfc>:22: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Epoch 1 : Train Loss = 0.5414423816427588 :  Test Loss = 0.24959914684295653\n",
            "Epoch 2 : Train Loss = 0.1821133549772203 :  Test Loss = 0.12759405076503755\n",
            "Epoch 3 : Train Loss = 0.12405825949832797 :  Test Loss = 0.10099260732531548\n",
            "Epoch 4 : Train Loss = 0.10607108416780829 :  Test Loss = 0.08845331221818924\n",
            "Epoch 5 : Train Loss = 0.09441171769797801 :  Test Loss = 0.07891470901668071\n",
            "Epoch 6 : Train Loss = 0.08459535798616707 :  Test Loss = 0.07068615704774857\n",
            "Epoch 7 : Train Loss = 0.07591087008826435 :  Test Loss = 0.06339624635875225\n",
            "Epoch 8 : Train Loss = 0.06815817696601152 :  Test Loss = 0.05690982900559902\n",
            "Epoch 9 : Train Loss = 0.06122243491001427 :  Test Loss = 0.05109430253505707\n",
            "Epoch 10 : Train Loss = 0.0550064846072346 :  Test Loss = 0.0458856163546443\n",
            "Epoch 11 : Train Loss = 0.04943234550766647 :  Test Loss = 0.04121451303362846\n",
            "Epoch 12 : Train Loss = 0.044430674446746705 :  Test Loss = 0.037022707238793376\n",
            "Epoch 13 : Train Loss = 0.03993972282763571 :  Test Loss = 0.03326405640691519\n",
            "Epoch 14 : Train Loss = 0.03590596082340926 :  Test Loss = 0.029884213395416737\n",
            "Epoch 15 : Train Loss = 0.03228087293822318 :  Test Loss = 0.026848969422280787\n",
            "Epoch 16 : Train Loss = 0.02902335241343826 :  Test Loss = 0.024121764302253722\n",
            "Epoch 17 : Train Loss = 0.02609480477962643 :  Test Loss = 0.021674697287380695\n",
            "Epoch 18 : Train Loss = 0.02346200933307409 :  Test Loss = 0.01947233434766531\n",
            "Epoch 19 : Train Loss = 0.021094935789238663 :  Test Loss = 0.01749103032052517\n",
            "Epoch 20 : Train Loss = 0.018967192433774472 :  Test Loss = 0.015716650616377593\n",
            "Epoch 21 : Train Loss = 0.01705406636884436 :  Test Loss = 0.014116520993411541\n",
            "Epoch 22 : Train Loss = 0.015334803397767246 :  Test Loss = 0.012684667157009244\n",
            "Epoch 23 : Train Loss = 0.013789455638732762 :  Test Loss = 0.011396869225427509\n",
            "Epoch 24 : Train Loss = 0.012400833007879555 :  Test Loss = 0.010237991251051426\n",
            "Epoch 25 : Train Loss = 0.011153286589309573 :  Test Loss = 0.00919946962967515\n",
            "Epoch 26 : Train Loss = 0.010032508293166757 :  Test Loss = 0.008267319854348898\n",
            "Epoch 27 : Train Loss = 0.009025886881398036 :  Test Loss = 0.007431264780461788\n",
            "Epoch 28 : Train Loss = 0.008122063173446804 :  Test Loss = 0.006680633104406297\n",
            "Epoch 29 : Train Loss = 0.007310609204228967 :  Test Loss = 0.006007614475674927\n",
            "Epoch 30 : Train Loss = 0.006582356726517901 :  Test Loss = 0.005404473724775016\n",
            "Epoch 31 : Train Loss = 0.005928834149148315 :  Test Loss = 0.004864088213071227\n",
            "Epoch 32 : Train Loss = 0.005342596263275482 :  Test Loss = 0.004378901980817318\n",
            "Epoch 33 : Train Loss = 0.004816831904929131 :  Test Loss = 0.003944249229971319\n",
            "Epoch 34 : Train Loss = 0.004345354380086064 :  Test Loss = 0.0035556646063923834\n",
            "Epoch 35 : Train Loss = 0.003922578688710928 :  Test Loss = 0.0032068078056909145\n",
            "Epoch 36 : Train Loss = 0.003543713517137803 :  Test Loss = 0.0028955550165846943\n",
            "Epoch 37 : Train Loss = 0.00320414850267116 :  Test Loss = 0.002617245260626078\n",
            "Epoch 38 : Train Loss = 0.0028999359658337196 :  Test Loss = 0.002367473905906081\n",
            "Epoch 39 : Train Loss = 0.0026273206813493743 :  Test Loss = 0.0021448435494676233\n",
            "Epoch 40 : Train Loss = 0.0023832813303451984 :  Test Loss = 0.001945417106617242\n",
            "Epoch 41 : Train Loss = 0.0021647724578506312 :  Test Loss = 0.0017669798689894378\n",
            "Epoch 42 : Train Loss = 0.0019692030330188573 :  Test Loss = 0.0016069564968347549\n",
            "Epoch 43 : Train Loss = 0.0017941060350858606 :  Test Loss = 0.00146422908292152\n",
            "Epoch 44 : Train Loss = 0.0016374233616224957 :  Test Loss = 0.001337339828023687\n",
            "Epoch 45 : Train Loss = 0.0014971939070674125 :  Test Loss = 0.0012241293443366885\n",
            "Epoch 46 : Train Loss = 0.0013717745304456911 :  Test Loss = 0.0011229958501644433\n",
            "Epoch 47 : Train Loss = 0.0012595605916430942 :  Test Loss = 0.0010327204247005284\n",
            "Epoch 48 : Train Loss = 0.0011592136931722053 :  Test Loss = 0.0009522068779915571\n",
            "Epoch 49 : Train Loss = 0.0010694883510877844 :  Test Loss = 0.0008801906078588217\n",
            "Epoch 50 : Train Loss = 0.0009892513468948891 :  Test Loss = 0.0008163015329046175\n",
            "Epoch 51 : Train Loss = 0.0009175335734908003 :  Test Loss = 0.0007592026377096772\n",
            "Epoch 52 : Train Loss = 0.0008534038124780636 :  Test Loss = 0.0007078963826643303\n",
            "Epoch 53 : Train Loss = 0.0007961144815199077 :  Test Loss = 0.000662754759832751\n",
            "Epoch 54 : Train Loss = 0.0007449078670470045 :  Test Loss = 0.0006227315549040213\n",
            "Epoch 55 : Train Loss = 0.0006991559797315858 :  Test Loss = 0.0005865987885044887\n",
            "Epoch 56 : Train Loss = 0.000658292580919806 :  Test Loss = 0.0005546046275412663\n",
            "Epoch 57 : Train Loss = 0.0006217732008663006 :  Test Loss = 0.0005264177758363076\n",
            "Epoch 58 : Train Loss = 0.0005891485815955093 :  Test Loss = 0.0005010557797504589\n",
            "Epoch 59 : Train Loss = 0.0005600033372611505 :  Test Loss = 0.00047866004751995206\n",
            "Epoch 60 : Train Loss = 0.0005339710309490329 :  Test Loss = 0.00045881723053753376\n",
            "Epoch 61 : Train Loss = 0.0005107142674241913 :  Test Loss = 0.0004409536297316663\n",
            "Epoch 62 : Train Loss = 0.0004899391413637204 :  Test Loss = 0.0004252305720001459\n",
            "Epoch 63 : Train Loss = 0.00047139836015412586 :  Test Loss = 0.0004112084483494982\n",
            "Epoch 64 : Train Loss = 0.00045483390413573945 :  Test Loss = 0.0003988804557593539\n",
            "Epoch 65 : Train Loss = 0.00044004798415699044 :  Test Loss = 0.00038801070186309514\n",
            "Epoch 66 : Train Loss = 0.00042684474593261256 :  Test Loss = 0.0003783702806686051\n",
            "Epoch 67 : Train Loss = 0.00041506175744143546 :  Test Loss = 0.0003696448664413765\n",
            "Epoch 68 : Train Loss = 0.00040455790011037604 :  Test Loss = 0.00036211763945175334\n",
            "Epoch 69 : Train Loss = 0.0003951794288950623 :  Test Loss = 0.000355425629823003\n",
            "Epoch 70 : Train Loss = 0.000386810253978183 :  Test Loss = 0.00034954339498654007\n",
            "Epoch 71 : Train Loss = 0.0003793350022751838 :  Test Loss = 0.00034434408007655293\n",
            "Epoch 72 : Train Loss = 0.00037267001045984216 :  Test Loss = 0.0003396559754037298\n",
            "Epoch 73 : Train Loss = 0.00036670355661772193 :  Test Loss = 0.0003359352980623953\n",
            "Epoch 74 : Train Loss = 0.0003614137540716911 :  Test Loss = 0.000332258858543355\n",
            "Epoch 75 : Train Loss = 0.00035667672583804234 :  Test Loss = 0.0003290017513791099\n",
            "Epoch 76 : Train Loss = 0.00035243429575348273 :  Test Loss = 0.00032623045553918927\n",
            "Epoch 77 : Train Loss = 0.0003486650631966768 :  Test Loss = 0.0003238276753108948\n",
            "Epoch 78 : Train Loss = 0.0003452875951261376 :  Test Loss = 0.00032150286860996855\n",
            "Epoch 79 : Train Loss = 0.00034228200382494835 :  Test Loss = 0.0003195173921994865\n",
            "Epoch 80 : Train Loss = 0.0003396044513065135 :  Test Loss = 0.00031790421489859\n",
            "Epoch 81 : Train Loss = 0.00033721251586393917 :  Test Loss = 0.00031664771522628145\n",
            "Epoch 82 : Train Loss = 0.00033507474514044586 :  Test Loss = 0.00031543948571197686\n",
            "Epoch 83 : Train Loss = 0.00033316985711280724 :  Test Loss = 0.00031434707052540033\n",
            "Epoch 84 : Train Loss = 0.0003314772427911521 :  Test Loss = 0.0003133839592919685\n",
            "Epoch 85 : Train Loss = 0.0003299675601301715 :  Test Loss = 0.0003126543990219943\n",
            "Epoch 86 : Train Loss = 0.0003286175440734951 :  Test Loss = 0.00031207086431095377\n",
            "Epoch 87 : Train Loss = 0.00032741268179961483 :  Test Loss = 0.0003113454702543095\n",
            "Epoch 88 : Train Loss = 0.0003263299686805112 :  Test Loss = 0.00031095787999220194\n",
            "Epoch 89 : Train Loss = 0.00032537286725710146 :  Test Loss = 0.00031049721728777514\n",
            "Epoch 90 : Train Loss = 0.00032451519882306456 :  Test Loss = 0.0003100586691289209\n",
            "Epoch 91 : Train Loss = 0.000323752410527959 :  Test Loss = 0.0003096873639151454\n",
            "Epoch 92 : Train Loss = 0.0003230757411874947 :  Test Loss = 0.00030944241443648935\n",
            "Epoch 93 : Train Loss = 0.0003224615969011211 :  Test Loss = 0.00030938803392928095\n",
            "Epoch 94 : Train Loss = 0.0003219236506629386 :  Test Loss = 0.0003091957842116244\n",
            "Epoch 95 : Train Loss = 0.0003214410874497844 :  Test Loss = 0.00030898657714715225\n",
            "Epoch 96 : Train Loss = 0.0003210059448683751 :  Test Loss = 0.00030881050624884663\n",
            "Epoch 97 : Train Loss = 0.000320610357055557 :  Test Loss = 0.0003084672716795467\n",
            "Epoch 98 : Train Loss = 0.0003202808870264562 :  Test Loss = 0.0003085690361331217\n",
            "Epoch 99 : Train Loss = 0.00031997103559115204 :  Test Loss = 0.0003086551645537838\n",
            "Epoch 100 : Train Loss = 0.00031970035902486416 :  Test Loss = 0.0003086367214564234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8lROgCU6BEoh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PLOT"
      ]
    },
    {
      "metadata": {
        "id": "qEax4t3y_hSx",
        "colab_type": "code",
        "outputId": "17ca9bdc-aae3-4ca2-cba1-91b4ec4d77ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "print(matplotlib.__version__)\n",
        "plt.plot(ts, label='TEST')\n",
        "plt.plot(tr, label='TRAIN')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oqRlbPI6xvsj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 0\n",
        "## Your goal is to:\n",
        "### Expand this model to receive multiple inputs and the output must be of size 2.\n",
        "Your network must receive 5 different numbers, and output the sumation of cossines of the numbers and the summation of the sines.\n",
        "\n",
        "### Add L2 and L1 regularizations directly on the loss function and include their coeficcients as hyper-parameters\n",
        "\n",
        "### Add comments to this code\n",
        "\n",
        "## Feel free to make changes, as long as they make the code clear and keep a similar structure"
      ]
    },
    {
      "metadata": {
        "id": "cQWnhrVomoIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating the data\n",
        "\n",
        "As described, the input will be five numbers"
      ]
    },
    {
      "metadata": {
        "id": "7WANOKmvmlno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loadTrainData():\n",
        "  x = np.random.rand(16000,5)\n",
        "  y = np.vstack([np.sum(np.sin(x),axis=1), np.sum(np.cos(x),axis=1)]).transpose()\n",
        "  return x , y\n",
        "\n",
        "def loadTestData():\n",
        "  x = np.random.rand(160,5)\n",
        "  y = np.vstack([np.sum(np.sin(x),axis=1), np.sum(np.cos(x),axis=1)]).transpose()\n",
        "  return x , y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRdxqz65ojES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating the model class\n",
        "Sine the input and output have changed, the model must be modified. The input is modified to accept 5 element arrays, and the output is now 2 elements.\n",
        "The regularization is added to the dense layers, since the regularization must be aplied to the weights, and the \"dense\" layer does not allow for the graph to get acces to the weights. Thus, the contrib regularizations have been used."
      ]
    },
    {
      "metadata": {
        "id": "4Fg-XsKFhc0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  \"\"\"\n",
        "  Tensor Flow fully connected network model. \n",
        "  `Model(learning_rate=0.0001, mini_batches=16, neurons=[4], reg_type=\"L2\", reg=0.01)`\n",
        "  Parameters\n",
        "  \n",
        "  learning_rate : the learning rate parameter for the optimization method\n",
        "  mini_batches : Number of elements in every batch for the learning phase\n",
        "  layers : list of integers representing the number of neurons on every layer\n",
        "  reg : list of regularization coeficients for L1 and L2\n",
        "  \"\"\"\n",
        "  def __init__(self,learning_rate=0.0001,\n",
        "               mini_batches=16,\n",
        "               layers=[4,4],\n",
        "               reg_type=\"L2\",\n",
        "               reg=0.01):\n",
        "    \n",
        "    self.lr = learning_rate\n",
        "    self.mb = mini_batches\n",
        "    \n",
        "    # Input and labels can only be float32\n",
        "    self.input = tf.placeholder(tf.float32, shape=(None, 5), name=\"X\")\n",
        "    self.label = tf.placeholder(tf.float32, shape=(None, 2), name=\"Y\")\n",
        "    \n",
        "    # Setup regularizer\n",
        "    if reg_type in [\"L1\",\"l1\"]:\n",
        "      regularizer = tf.contrib.layers.l1_regularizer(reg)\n",
        "    else:\n",
        "      regularizer = tf.contrib.layers.l2_regularizer(reg)\n",
        "    \n",
        "    # Initialization of model\n",
        "    self.output = self.input\n",
        "    for neurons in layers:\n",
        "      self.output = tf.layers.dense(self.output, \n",
        "                                    neurons, \n",
        "                                    kernel_regularizer=regularizer)\n",
        "      \n",
        "    self.output = tf.layers.dense(self.output, 2) # last dense layer\n",
        "    \n",
        "    # Definition of the loss\n",
        "    self.loss = (self.output - self.label) * (self.output - self.label)\n",
        "    self.loss = tf.reduce_mean(self.loss)\n",
        "    \n",
        "  def get_loss(self):\n",
        "    \"\"\"loss getter method\"\"\"\n",
        "    return self.loss\n",
        "  \n",
        "  def get_xy(self):\n",
        "    \"\"\"input and label getter\"\"\"\n",
        "    return self.input, self.label\n",
        "  \n",
        "  def get_output(self):\n",
        "    \"\"\"output getter\"\"\"\n",
        "    return self.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMKYUetMocJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "  \"\"\"\n",
        "  Learning algorithm for a TensorFlow model\n",
        "  `Optimizer(model)`\n",
        "  model : a fully connected TensorFlow neural network\n",
        "  \"\"\"\n",
        "  def __init__(self,model):\n",
        "    self.model     = model\n",
        "    self.loss      = model.get_loss()\n",
        "    self.X, self.Y = model.get_xy()\n",
        "    self.mb        = model.mb # Mini batches\n",
        "    self.output    = model.get_output()\n",
        "    \n",
        "    # Using GD as optimizier algorithm\n",
        "    self.opt       = tf.train.GradientDescentOptimizer(learning_rate = model.lr)\n",
        "    self.optAction = self.opt.minimize(self.loss) # minimize loss\n",
        "    \n",
        "  def batching (self, size):\n",
        "    \"\"\"Return the indexes of the input to be used for minibatch\"\"\"\n",
        "    r=np.arange(int(size))\n",
        "    np.random.shuffle(r)\n",
        "    return r\n",
        "  \n",
        "  def train (self, dataX, dataY, verbose):\n",
        "    \"\"\"Training of the NN\"\"\"\n",
        "    i     = 0 # A counter for the number of data processed\n",
        "    loss  = 0 \n",
        "    count = 0 # Another counter, for the number of minibatches required\n",
        "    batchOrder = self.batching(len(dataX)) # Get indexes of batch data\n",
        "#     print (batchOrder)\n",
        "    while (i+self.mb <= len(dataX)): # for all elements in the input \n",
        "      \n",
        "      # get the batch data\n",
        "      mbX, mbY   = dataX[batchOrder[i:i+self.mb]] , dataY[batchOrder[i:i+self.mb]]\n",
        "      \n",
        "      # run optimization\n",
        "      _ , mbLoss = self.sess.run([self.optAction, self.loss],\n",
        "                                   feed_dict={\n",
        "                                       self.X:mbX,\n",
        "                                       self.Y:mbY\n",
        "                                   })\n",
        "      if verbose>1:\n",
        "        print(\"\\t Inner loss: \"+str(mbLoss))\n",
        "          \n",
        "      loss  += mbLoss\n",
        "      i     += self.mb\n",
        "      count += 1\n",
        "    loss = loss / count\n",
        "    return loss\n",
        "  \n",
        "  def test  (self, dataX, dataY, verbose):\n",
        "    i     = 0 # A counter for the number of data processed\n",
        "    loss  = 0\n",
        "    count = 0 # Another counter, for the number of minibatches required\n",
        "    batchOrder = self.batching(len(dataX)) # Get indexes of batch data\n",
        "    \n",
        "    while (i+self.mb <= len(dataX)): # for all elements in the input \n",
        "      \n",
        "      # get test data for the batch  \n",
        "      mbX, mbY   = dataX[i:i+self.mb] , dataY[i:i+self.mb]\n",
        "      \n",
        "      # get the loss on the test set\n",
        "      mbLoss = self.sess.run(self.loss,\n",
        "                                   feed_dict={\n",
        "                                       self.X:mbX,\n",
        "                                       self.Y:mbY\n",
        "                                   })          \n",
        "      loss  += mbLoss\n",
        "      i     += self.mb\n",
        "      count += 1\n",
        "    loss = loss / count\n",
        "    return loss\n",
        "  \n",
        "  def run   (self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
        "    \"\"\"Run the learning phase and return the test and train history\"\"\"\n",
        "    historyTR = []\n",
        "    historyTS = []\n",
        "    with tf.Session() as self.sess:\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      lossTS = self.test  (testX , testY, verbose)\n",
        "      historyTR.append(lossTS)\n",
        "      historyTS.append(lossTS)\n",
        "      for i in range(epochs):\n",
        "        \n",
        "        lossTR = self.train (dataX , dataY, verbose)\n",
        "        lossTS = self.test  (testX , testY, verbose)\n",
        "        if verbose > 0:\n",
        "          print(\"Epoch \" +str(i+1)+\" : Train Loss = \" + str(lossTR)+\" :  Test Loss = \" + str(lossTS))\n",
        "        historyTR.append(lossTR)\n",
        "        historyTS.append(lossTS)\n",
        "    return historyTR, historyTS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8y1fLKK3pHjo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ]
    },
    {
      "metadata": {
        "id": "xWGIEpmno7VQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1875
        },
        "outputId": "7e892079-f123-4471-d0d9-61a4c861c735"
      },
      "cell_type": "code",
      "source": [
        "x  , y  = loadTrainData ()\n",
        "xt , yt = loadTestData  ()\n",
        "\n",
        "model  = Model ()\n",
        "opt    = Optimizer (model)\n",
        "tr, ts = opt.run (x, y, xt, yt, 100, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Epoch 1 : Train Loss = 7.072589210271835 :  Test Loss = 2.8836764097213745\n",
            "Epoch 2 : Train Loss = 1.2866219456493855 :  Test Loss = 0.5945340454578399\n",
            "Epoch 3 : Train Loss = 0.5046668979525566 :  Test Loss = 0.45388016402721404\n",
            "Epoch 4 : Train Loss = 0.4437495696246624 :  Test Loss = 0.4245546042919159\n",
            "Epoch 5 : Train Loss = 0.41993327014148235 :  Test Loss = 0.40474575757980347\n",
            "Epoch 6 : Train Loss = 0.4006376529186964 :  Test Loss = 0.3877517953515053\n",
            "Epoch 7 : Train Loss = 0.383642946138978 :  Test Loss = 0.37260630130767824\n",
            "Epoch 8 : Train Loss = 0.3684627807363868 :  Test Loss = 0.35903133302927015\n",
            "Epoch 9 : Train Loss = 0.35480024802684784 :  Test Loss = 0.3466470569372177\n",
            "Epoch 10 : Train Loss = 0.34241489925980567 :  Test Loss = 0.335419724881649\n",
            "Epoch 11 : Train Loss = 0.3311268806904554 :  Test Loss = 0.32513405829668046\n",
            "Epoch 12 : Train Loss = 0.32079011572897437 :  Test Loss = 0.3156635478138924\n",
            "Epoch 13 : Train Loss = 0.31127964286506177 :  Test Loss = 0.30695459097623823\n",
            "Epoch 14 : Train Loss = 0.30249869838356974 :  Test Loss = 0.2988592818379402\n",
            "Epoch 15 : Train Loss = 0.2943481870293617 :  Test Loss = 0.2913081422448158\n",
            "Epoch 16 : Train Loss = 0.28675052492320535 :  Test Loss = 0.284254652261734\n",
            "Epoch 17 : Train Loss = 0.2796614848747849 :  Test Loss = 0.27765838503837587\n",
            "Epoch 18 : Train Loss = 0.2730286786407232 :  Test Loss = 0.2713702693581581\n",
            "Epoch 19 : Train Loss = 0.2667935596331954 :  Test Loss = 0.2654903128743172\n",
            "Epoch 20 : Train Loss = 0.26091704580187797 :  Test Loss = 0.2599716976284981\n",
            "Epoch 21 : Train Loss = 0.25536255629360677 :  Test Loss = 0.2546886831521988\n",
            "Epoch 22 : Train Loss = 0.2500887689217925 :  Test Loss = 0.24969296753406525\n",
            "Epoch 23 : Train Loss = 0.2451107096672058 :  Test Loss = 0.244952791929245\n",
            "Epoch 24 : Train Loss = 0.24036153164505958 :  Test Loss = 0.2404335543513298\n",
            "Epoch 25 : Train Loss = 0.2358335173651576 :  Test Loss = 0.2360834911465645\n",
            "Epoch 26 : Train Loss = 0.23151329378038646 :  Test Loss = 0.23195018023252487\n",
            "Epoch 27 : Train Loss = 0.22737149818241595 :  Test Loss = 0.22797856628894805\n",
            "Epoch 28 : Train Loss = 0.22339289593696593 :  Test Loss = 0.22419022023677826\n",
            "Epoch 29 : Train Loss = 0.2195807281769812 :  Test Loss = 0.22050614207983016\n",
            "Epoch 30 : Train Loss = 0.21591425151750446 :  Test Loss = 0.21694037690758705\n",
            "Epoch 31 : Train Loss = 0.21237007819116116 :  Test Loss = 0.21347733065485955\n",
            "Epoch 32 : Train Loss = 0.20895002268999815 :  Test Loss = 0.21015894263982773\n",
            "Epoch 33 : Train Loss = 0.20563605318963527 :  Test Loss = 0.20693743005394935\n",
            "Epoch 34 : Train Loss = 0.2024261827506125 :  Test Loss = 0.20381410121917726\n",
            "Epoch 35 : Train Loss = 0.1993121145069599 :  Test Loss = 0.20075868740677832\n",
            "Epoch 36 : Train Loss = 0.19627433466911315 :  Test Loss = 0.19784616231918334\n",
            "Epoch 37 : Train Loss = 0.193333282135427 :  Test Loss = 0.19492679685354233\n",
            "Epoch 38 : Train Loss = 0.19045290556550026 :  Test Loss = 0.19208435863256454\n",
            "Epoch 39 : Train Loss = 0.18763955491036177 :  Test Loss = 0.18935266137123108\n",
            "Epoch 40 : Train Loss = 0.1848909470140934 :  Test Loss = 0.1866455592215061\n",
            "Epoch 41 : Train Loss = 0.18219955978915095 :  Test Loss = 0.18396648913621902\n",
            "Epoch 42 : Train Loss = 0.17955399291217328 :  Test Loss = 0.18134324699640275\n",
            "Epoch 43 : Train Loss = 0.1769585750065744 :  Test Loss = 0.1788216568529606\n",
            "Epoch 44 : Train Loss = 0.17440666295588017 :  Test Loss = 0.17625414654612542\n",
            "Epoch 45 : Train Loss = 0.17189707406237723 :  Test Loss = 0.17377417534589767\n",
            "Epoch 46 : Train Loss = 0.1694204090088606 :  Test Loss = 0.17133673951029776\n",
            "Epoch 47 : Train Loss = 0.16697856702283026 :  Test Loss = 0.16889969110488892\n",
            "Epoch 48 : Train Loss = 0.1645637455433607 :  Test Loss = 0.1664982683956623\n",
            "Epoch 49 : Train Loss = 0.162168915130198 :  Test Loss = 0.1640944689512253\n",
            "Epoch 50 : Train Loss = 0.15979916977509856 :  Test Loss = 0.16180749535560607\n",
            "Epoch 51 : Train Loss = 0.15746421241015196 :  Test Loss = 0.1594267964363098\n",
            "Epoch 52 : Train Loss = 0.15513877369835974 :  Test Loss = 0.15708614960312844\n",
            "Epoch 53 : Train Loss = 0.15283662531897427 :  Test Loss = 0.1547890290617943\n",
            "Epoch 54 : Train Loss = 0.15054361798241733 :  Test Loss = 0.15251204147934913\n",
            "Epoch 55 : Train Loss = 0.1482586433775723 :  Test Loss = 0.15025099739432335\n",
            "Epoch 56 : Train Loss = 0.14599262875691058 :  Test Loss = 0.14795163720846177\n",
            "Epoch 57 : Train Loss = 0.14373413353413345 :  Test Loss = 0.1457257091999054\n",
            "Epoch 58 : Train Loss = 0.14148667957633734 :  Test Loss = 0.1434388391673565\n",
            "Epoch 59 : Train Loss = 0.13924471129104496 :  Test Loss = 0.1411793790757656\n",
            "Epoch 60 : Train Loss = 0.1370081870108843 :  Test Loss = 0.13895680531859397\n",
            "Epoch 61 : Train Loss = 0.13477924821153284 :  Test Loss = 0.1367243595421314\n",
            "Epoch 62 : Train Loss = 0.1325497017838061 :  Test Loss = 0.13448745086789132\n",
            "Epoch 63 : Train Loss = 0.1303260238841176 :  Test Loss = 0.13223332017660142\n",
            "Epoch 64 : Train Loss = 0.12810512440279126 :  Test Loss = 0.12999845445156097\n",
            "Epoch 65 : Train Loss = 0.12588455656543374 :  Test Loss = 0.12777431458234786\n",
            "Epoch 66 : Train Loss = 0.12366123745962977 :  Test Loss = 0.12551238834857942\n",
            "Epoch 67 : Train Loss = 0.1214522959664464 :  Test Loss = 0.1233020693063736\n",
            "Epoch 68 : Train Loss = 0.11922977644205093 :  Test Loss = 0.12104836478829384\n",
            "Epoch 69 : Train Loss = 0.11701792695000768 :  Test Loss = 0.1188222099095583\n",
            "Epoch 70 : Train Loss = 0.11480053693801165 :  Test Loss = 0.11659675762057305\n",
            "Epoch 71 : Train Loss = 0.11258672019466757 :  Test Loss = 0.11434962898492813\n",
            "Epoch 72 : Train Loss = 0.11036629308201372 :  Test Loss = 0.1121002934873104\n",
            "Epoch 73 : Train Loss = 0.10815771028026938 :  Test Loss = 0.10986368954181672\n",
            "Epoch 74 : Train Loss = 0.10593508977815509 :  Test Loss = 0.10761109180748463\n",
            "Epoch 75 : Train Loss = 0.10373164349980653 :  Test Loss = 0.10540774539113044\n",
            "Epoch 76 : Train Loss = 0.10152267192304135 :  Test Loss = 0.10315836519002915\n",
            "Epoch 77 : Train Loss = 0.0993116258122027 :  Test Loss = 0.10093337744474411\n",
            "Epoch 78 : Train Loss = 0.09711064266227186 :  Test Loss = 0.09870293773710728\n",
            "Epoch 79 : Train Loss = 0.09490562120266259 :  Test Loss = 0.09649187177419663\n",
            "Epoch 80 : Train Loss = 0.0927164469063282 :  Test Loss = 0.09424027465283871\n",
            "Epoch 81 : Train Loss = 0.0905229610837996 :  Test Loss = 0.09201638475060463\n",
            "Epoch 82 : Train Loss = 0.08833988464437426 :  Test Loss = 0.08978582359850407\n",
            "Epoch 83 : Train Loss = 0.08615964757278562 :  Test Loss = 0.08756589032709598\n",
            "Epoch 84 : Train Loss = 0.08398965705372394 :  Test Loss = 0.08538788035511971\n",
            "Epoch 85 : Train Loss = 0.08182875399664044 :  Test Loss = 0.08316570892930031\n",
            "Epoch 86 : Train Loss = 0.07968361198157072 :  Test Loss = 0.08099293150007725\n",
            "Epoch 87 : Train Loss = 0.07754734441265464 :  Test Loss = 0.07882109060883521\n",
            "Epoch 88 : Train Loss = 0.07542286608181893 :  Test Loss = 0.07664806731045246\n",
            "Epoch 89 : Train Loss = 0.07331167430989444 :  Test Loss = 0.07451169230043889\n",
            "Epoch 90 : Train Loss = 0.07122069402039051 :  Test Loss = 0.07237924709916115\n",
            "Epoch 91 : Train Loss = 0.06914475371688604 :  Test Loss = 0.07025248259305954\n",
            "Epoch 92 : Train Loss = 0.06708544958010317 :  Test Loss = 0.06815085932612419\n",
            "Epoch 93 : Train Loss = 0.06505227522365749 :  Test Loss = 0.06607743948698044\n",
            "Epoch 94 : Train Loss = 0.06303369761444629 :  Test Loss = 0.06403518617153167\n",
            "Epoch 95 : Train Loss = 0.06104367297515273 :  Test Loss = 0.061989374458789825\n",
            "Epoch 96 : Train Loss = 0.05907132451981306 :  Test Loss = 0.06000217348337174\n",
            "Epoch 97 : Train Loss = 0.05713357677310705 :  Test Loss = 0.05799784809350968\n",
            "Epoch 98 : Train Loss = 0.055218550216406585 :  Test Loss = 0.05604086052626371\n",
            "Epoch 99 : Train Loss = 0.053328261913731696 :  Test Loss = 0.054109424352645874\n",
            "Epoch 100 : Train Loss = 0.05148060110211372 :  Test Loss = 0.05221879873424769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OtX-54WTpNgs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting the learning curve"
      ]
    },
    {
      "metadata": {
        "id": "Z16dU8Pso_Co",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "777e500d-a5f1-4fc5-dbbd-b00c24e730c4"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "print(matplotlib.__version__)\n",
        "plt.plot(ts, label='TEST')\n",
        "plt.plot(tr, label='TRAIN')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHVtJREFUeJzt3X2MHHed5/H3tx56emYc7Hgy5zyY\nxD5dyJJzwEHDHVmOvRWGI7BO2IiVcLjlYIlkkI5zFoFyiXIcWQkQcGEX0OaQosAme4dsRDa65SJu\nIckSodORwAQMTmKyIcQJkwd74iRO7Hnorqrv/VHVPQ/uts10j3uq5/OSrK6qfvoVFT6/33zrV9Xm\n7oiISPkFvW6AiIh0hwJdRKRPKNBFRPqEAl1EpE8o0EVE+oQCXUSkTyjQRUT6hAJdRKRPKNBFRPpE\ndDq/7KyzzvJNmzadzq8UESm9hx566AV3Hz3Z605roG/atInx8fHT+ZUiIqVnZk+dyutUchER6RMK\ndBGRPqFAFxHpE6e1hi4i0k69XmdiYoKZmZleN6VnqtUqGzduJI7jJb1fgS4iK8LExARnnHEGmzZt\nwsx63ZzTzt05fPgwExMTbN68eUmfoZKLiKwIMzMzjIyMrMowBzAzRkZGOvoLRYEuIivGag3zhk73\nvxSBvve+PTxwx3/pdTNERFa0k9bQzeybwHbgkLtvKbb9N+AKoAY8AfyZu7+8XI2c2f99fu+FHwCf\nXa6vEJFV7vDhw2zbtg2A559/njAMGR3NL878xS9+wRvf+Mbma3fs2MH111/P3Xffzac//WmyLKNe\nr3Pttdfywgsv8J3vfAeAffv2cckllwDwkY98hF27di3rPpzKSdHbgb8G/nbetnuAG9w9MbMvAjcA\n/7n7zcul4SBVn12ujxcRYWRkhL179wJw0003sWbNGj71qU8BsGbNmuZzDfV6nZ07d/KTn/yEjRs3\nMjs7y4EDB7jooou48cYb275vOZ205OLuPwJeXLTtB+6eFKsPABuXoW1NWVilanVwX86vERE5Za++\n+ipJkjAyMgLAwMAAF110UU/b1I1pix8Bvt2Fz2kriwbyhWQG4sHl/CoRWQH+4n8/wqPPvtLVz7z4\n3NfwmSv+5ZLeOz09zdatW5vrN9xwA+9///u58sorueCCC9i2bRvbt2/n6quvJgh6d2qyo0A3sxuB\nBPjWCV6zE9gJcP755y/pe7IwD3GvTWEKdBE5zQYHB1uWTm677Tb27dvHvffey80338w999zD7bff\nfvobWFhyoJvZh8lPlm5zb18LcfdbgVsBxsbGllQz8agKQH12isrwyFI+QkRKZKkj6V645JJLuOSS\nS/jgBz/I5s2bexroS/rbwMwuB64DrnT3qe426XiNQE9ry/5VIiKn5OjRo9x///3N9b1793LBBRf0\nrkGc2rTF3cAfAmeZ2QTwGfJZLQPAPcVE+Afc/WPL1UiP8jJLMqtAF5HTb3EN/fLLL+fGG2/kS1/6\nEh/96EcZHBxkeHi4p6NzOIVAd/erW2z+xjK0pb0i0DMFuoicBjfddNOC9TRNW77ue9/73gk/5+jR\no91q0ikpxZWiHjdKLtM9bomIyMpVikC3eAiAVCN0EZG2yhHolXyEntU1QhcRaacUgd6oobtmuYiI\ntFWKQA8qxUlR1dBFRNoqRaA3augkCnQRkXZKEeiNEbqrhi4iy+Tw4cNs3bqVrVu3cvbZZ3Peeec1\n182MrVu3smXLFq644gpefnnh3cK/8pWvUK1WOXLkSHPb/fffz/bt2wG4/fbbCYKAX/7yl83nt2zZ\nwoEDB7q6D6UI9DCukrmBSi4iskwat8/du3cvH/vYx/jEJz7RXB8eHmbv3r08/PDDrF+/nltuuWXB\ne3fv3s2b3/xm7rrrrrafv3HjRj73uc8t6z6UItDjKGSGCq6Si4j02GWXXcYzzzzTXH/iiSc4evQo\nn/3sZ9m9e3fb923fvp1HHnmExx57bNna1o3b5y67KDRmiPPb54pI//s/18Pz+7r7mWdfAu/+Qkcf\nkaYp9913H9dcc01z2549e9ixYwdve9vbeOyxxzh48CAbNmw47r1BEHDdddfx+c9/njvuuKOjdrRT\njhF6EDBDBVMNXUR6oHEvl7PPPpuDBw/yzne+s/nc7t272bFjB0EQ8L73va/583OtfOADH+CBBx7g\nySefXJZ2lmaEPu0DDGqELrI6dDiS7rbG/dCnpqZ417vexS233MKuXbvYt28fjz/+eDPga7Uamzdv\n5uMf/3jLz4miiE9+8pN88YtfXJZ2lmOEHhqzVDAFuoj00NDQEF/72tf48pe/TJIk7N69m5tuuokD\nBw5w4MABnn32WZ599lmeeuqptp/x4Q9/mHvvvZfJycmut68UgR4FATPEBDopKiI9dumll/KGN7yB\n3bt3s2fPHq666qoFz1911VXs2bOn7fsrlQq7du3i0KFDXW+bneDHhrpubGzMx8fHf+f3Tbw0xVN/\n+Q5ed1aF0Wvv737DRKTn9u/fz+tf//peN6PnWv3vYGYPufvYyd5bihF6HOYnRYNUJRcRkXZKFOgx\noQJdRKStUgR6Pg99QIEu0udOZwl4Jep0/0sR6HEQMOMVwnS2100RkWVSrVY5fPjwqg11d+fw4cNU\nq9Ulf0Zp5qHPUCHKNEIX6VcbN25kYmJiWabzlUW1WmXjxo1Lfn85Aj0wpqkQaYQu0rfiOGbz5s29\nbkaplaLkYmbUqBCQQlrvdXNERFakUgQ6QD0YKBb0M3QiIq2UJtBrQXGioK46uohIKycNdDP7ppkd\nMrOH521bb2b3mNnjxeOZy9tMSKwYoevyfxGRlk5lhH47cPmibdcD97n7hcB9xfqymiu5KNBFRFo5\naaC7+4+AFxdtfi/QuEP7HcAfd7ldx0maJRcFuohIK0utoW9w9+eK5eeB43+eo2BmO81s3MzGO5lf\nmoSNkotq6CIirXR8UtTzy7raXtrl7re6+5i7j42Oji75e5o1dM1yERFpaamBftDMzgEoHrt/Y99F\n0lCzXERETmSpgf5d4EPF8oeAv+9Oc9rLoiLQVXIREWnpVKYt7gZ+DFxkZhNmdg3wBeCdZvY48I5i\nfVmloWa5iIicyEnv5eLuV7d5aluX23JCaTiYLyjQRURaKs2Volmjhq4Li0REWipPoEc6KSoiciKl\nCfQojKgTadqiiEgbJQp0Y5YBzXIREWmjRIEeMGsVnRQVEWmjNIFeCQNmUKCLiLRTmkCPgvx3RTXL\nRUSktfIEehgw4xXNchERaaM0gR6H+Q9Fq+QiItJaaQI9CooRukouIiItlSbQ49CYVslFRKSt0gR6\nFBpTHuvCIhGRNkoT6HEYMO0VXBcWiYi0VKpA1zx0EZH2ShPo+Tz0AQW6iEgb5Qn0MGCGGEtnIct6\n3RwRkRWnNIEeh5ZPWwTdoEtEpIXSBHoUBEyjn6ETEWmnPIEeFvdyAV1cJCLSQmkCvRIGzHicr+ji\nIhGR45Qm0PMReqPkoouLREQWK0+gB8G8kotG6CIii5Um0BfMctFJURGR43QU6Gb2CTN7xMweNrPd\nZlbtVsMWa8xDBxToIiItLDnQzew8YBcw5u5bgBDY0a2GLRYHNjdtUbNcRESO02nJJQIGzSwChoBn\nO29Smy8K59XQNctFROQ4Sw50d38GuBl4GngOOOLuP+hWwxZbWEPXLBcRkcU6KbmcCbwX2AycCwyb\n2Z+2eN1OMxs3s/HJycklNzQONctFROREOim5vAN40t0n3b0O3AX8/uIXufut7j7m7mOjo6NL/rIF\nV4rqpKiIyHE6CfSngbeY2ZCZGbAN2N+dZh0vCgJqRDimQBcRaaGTGvqDwJ3Az4B9xWfd2qV2HScO\nDTDSsKqSi4hIC1Enb3b3zwCf6VJbTigK874nCatEGqGLiBynPFeKBgZAGuhXi0REWilNoDdG6PWg\nqguLRERaKE2g5zV0SIIBXVgkItJCiQK9qKEHA7qwSESkhdIEelTU0GuBZrmIiLRSmkAPi0CvW0Un\nRUVEWihNoJsZcWjUTbNcRERaKU2gQ3616KwNqOQiItJCuQI9NGoaoYuItFSqQK+EAbOohi4i0kqp\nAj0foVdUchERaaFcgR4EzBKDp5AmvW6OiMiKUqpAj0PLSy6gUbqIyCKlCvQoLEbooEAXEVmkXIEe\nGDOuQBcRaaVUgR6HwdwPRSezvW2MiMgKU7JAnzdC19RFEZEFShXoURgw7cWPLGmELiKyQKkCfcEI\nXTV0EZEFShXoURAwrUAXEWmpVIEeh8ZUpkAXEWmlVIGej9AbNXQFuojIfOUK9NA4lmnaoohIK6UK\n9EoYMJWF+YqmLYqILNBRoJvZOjO708x+ZWb7zeyybjWslSg0ptJGDV0jdBGR+aIO3/9V4B/c/U/M\nrAIMdaFNbUXzR+iqoYuILLDkQDeztcAfAB8GcPcaUOtOs1qLA+OoRugiIi11UnLZDEwCf2NmPzez\n28xsePGLzGynmY2b2fjk5GQHX5eP0JPMIRyARDV0EZH5Ogn0CHgT8HV3vxQ4Bly/+EXufqu7j7n7\n2OjoaAdfl9fQ65lDVNUIXURkkU4CfQKYcPcHi/U7yQN+2cRBQD3NIK6qhi4issiSA93dnwd+a2YX\nFZu2AY92pVVtRKHhDh4NQF2BLiIyX6ezXP4T8K1ihstvgD/rvEntxWHe/3g4gGmELiKyQEeB7u57\ngbEuteWk4tDy71UNXUTkOKW6UjQKGiN01dBFRBYrVaA3RuhZOKBAFxFZpFSBHhU19CysKNBFRBYp\nV6AH80foqqGLiMxXqkBvzHJJg6rutigiskg5Az2saIQuIrJIqQI9Kk6KpoFOioqILFaqQI8V6CIi\nbZUq0Bvz0JOgmOXi3uMWiYisHOUK9GKEntgAeAZZ0uMWiYisHKUK9MZJ0SRo/FC0yi4iIg2lCvTG\nPPR6I9B1x0URkaZSBXpjhF43jdBFRBYrZ6AzkG/QXHQRkaZSBXrjpGiz5KLfFRURaSpVoMfFtMUa\njUDXCF1EpKFUgd4YodeI8w2qoYuINJUz0HVSVETkOKUK9EbJZcY1bVFEZLFyBXpU1NCt+ClUjdBF\nRJpKFeiNC4uaI3SdFBURaSpVoDfmoc+iaYsiIouVKtDDwDCD2eYsF43QRUQaOg50MwvN7Odmdnc3\nGnQycRAw7Zq2KCKyWDdG6NcC+7vwOackCo3ZrHFSVCN0EZGGjgLdzDYCfwTc1p3mnFwUGPXMIdIP\nRYuIzNfpCP0rwHVA1oW2nJI4DIpAH9AIXURkniUHupltBw65+0Mned1OMxs3s/HJycmlfl1THAYk\naZaP0FVDFxFp6mSE/lbgSjM7AOwB3m5m/3Pxi9z9Vncfc/ex0dHRDr4uF4VGkroCXURkkSUHurvf\n4O4b3X0TsAP4R3f/0661rI25kosCXURkvlLNQ4f8pGheclENXURkvqgbH+Lu9wP3d+OzTiYKA+oq\nuYiIHKd0I/Q4NOppBnFVd1sUEZmndIEeBUaSaZaLiMhipQv0uFlyUQ1dRGS+Ugb63Dx0XSkqItJQ\nukCPQiNpTlvUCF1EpKF8gR5olouISCulC/Q41Dx0EZFWShfo+Tz0DOLB/G6L7r1ukojIilC6QI8D\nm5vlgkNa73WTRERWhPIFehjMzUMH1dFFRAqlC/QFd1sEBbqISKF0gR43augKdBGRBUoX6FGzht4I\ndM10ERGBEgb6mmrEdD0lCSr5Bo3QRUSAEgb6yHAe5EfT4s6/uuOiiAhQwkBfPzwAwJF6mG/QCF1E\nBChloOcj9CP1oumqoYuIACUM9JE1eaC/WGsEuu64KCICJQz0xgj9pVmN0EVE5itdoJ85VMEMDs9Y\nvkE1dBERoISBHgbGusGYSQW6iMgCpQt0yMsuzUDXtEUREaCkgT4yPMDBqWJFI3QREaCkgb5+uMKh\nY8V90HVSVEQE6CDQzey1ZvZDM3vUzB4xs2u72bATWb+mwotTdf1QtIjIPFEH702AT7r7z8zsDOAh\nM7vH3R/tUtvaGhmu8NJUDV9bxTRCFxEBOhihu/tz7v6zYvlVYD9wXrcadiLrhytkDh4OqIYuIlLo\nSg3dzDYBlwIPtnhup5mNm9n45ORkN76ueXFRGuqHokVEGjoOdDNbA/wd8Ofu/sri5939Vncfc/ex\n0dHRTr8OyGe5ANRtIP+haBER6SzQzSwmD/Nvuftd3WnSyZ05HANQt1gjdBGRQiezXAz4BrDf3f+y\ne006ucYIfZaKaugiIoVORuhvBT4IvN3M9hb/3tOldp1QY4Q+67ECXUSksORpi+7+fwHrYltO2UAU\ncsZAxLTHkBzrRRNERFacUl4pCvnFRceySDV0EZFCeQN9uAh0zXIREQFKHOgjwxVeTTRCFxFp6OTS\n/55aP1zh1SSEQCdFRUSg1IE+wCv1EA9me3NmVkRkhSl1yWXKIyyZhiztdXNERHqutIG+frjCb7Jz\n85WDj/S2MSIiK0B5A31NhZ9mF+UrT/+4t40REVkBShvoI8MVnmGU6aFz4an/1+vmiIj0XGkDvXEL\n3YPrLs1H6O49bpGISG+VNtAbN+g6sOaNcPQgvPibHrdIRKS3Shvog5WQwTjkV/GWfIPKLiKyypU2\n0CEvu/xTei4MjejEqIiseqUO9JE1FQ5P1eH8yzRCF5FVr9SBvn64wovHanmgv/QkvPJcr5skItIz\n5Q70oSLQL/j9fMPTGqWLyOpV7kAfrvDC0VleOfP1UFkDT6mOLiKrV6kD/W2vGyXJnCtueYCjo5fm\ndXTNRxeRVarUgf5vXzfKt3e+hVqS8d+ffi0ceoTZv7qU2vf/Kzz9IEy9qIAXkVXD/DQG3tjYmI+P\nj3f9c188VuO6b4/zz564k3cHD3JZ8CiRZQBM2xCvDJzNzOAG0uGz4YxziNduoHrmOQytP4ehdRuw\n4VGoroOg1P2biPQpM3vI3cdO+rp+CHQAd+fXh47yxORRJp59hui3PyZ+9bcMTT3DutrzjGSH2WAv\ncRZHCO34fU4JOGZrOBa+hpl4LfX4NSSVtaQDa2HgNdjgWqKhtYSD64iH1lIZXsvA8GsYHF5HdXgt\nQfUMiCrLsm8isrqdaqCX9gcuFjMzLtxwBhduOAO2nAMs3PeZesrhYzUePjLFqy8dZPrFZ6kdeZ7s\n2GFs6gXC6cNEtSMM1I8wNHuEwemDrPEnWMsxzmCqZSewWJ2IaarMWpXZoEotqFIPBknCQZKwShZW\nSaNBPKpCVIVoEOJBLB7E4ipBZZAgrhLEg0SVKmElf4wGBokrVaKBKnGcr1cGqsSVKqa/KkSk0DeB\nfjLVOOS8dYOct24QLhgBLj7pe9ydqVrKCzN1jh09wvSrLzF79CVqU0dIp14mmTkKM6+QzR6F2jGs\nPkVYP0qYTBOm08TpFFE2QyV5mcFsmorXGPBZBpmlQo2Kdf7DHDUPqRNTt4g6MQkRicUkFpFaTFo8\nZhaRBhGZxWRBhFtMFsR4EBX/8mWCGML80YtlC2MI5h6DMMaihctBEGFhsR7GhFH++jCMCKKIIKwQ\nFMth8Xz+mP8LwpAoqhCEAVEYEljeSYvIqeso0M3scuCrQAjc5u5f6EqrVggzY3ggYngggrWDwNld\n++w0c6Zqs8xMT1GfmaI2M0Vtdop0dpq0Nk1SmyGtTZHWZ8nqM2T1GTyZxZNZqM9iaQ1Pa5DMYlkd\nS2tYVsOyOkFWJ0hrmCeEWZ3Qa0RZjYFkitDrhJ4QekpInchTQhIiUiISIk+JSAlO4S+S5VL3kDoB\nKQEpIVnxmFpARkBGSGphsRyQNZabjxFuAZnlr3ULcQvwYvvc+rzngggI8CDfZkG+HQshiCAIwfLn\nzcJiPcTCsHhN/h4swsLiNWGEBYZZhBXP589FWBQRNLYFEUGYLwdBmHeMQZh3gGHx2FiPIoJgbnsY\nBHknGUR5xxgEhIGpQ1yllhzoZhYCtwDvBCaAn5rZd9390W41rp+FgTFUrTJUrQLre92c43iakKV1\n6rVZ0iQhrddIkhppWiep18jqddK0Tpbmz3mWvz5L6mRpiqd1srSOp0n+XJLgnkCa4GkdshTP5pYb\n6+YpZAmkdfAMPMWK5fy5NH/0/DHI8uWAFMtSzDOMlNBTzBMCn8WylIAM87xrCJqPKc0uYt5y3g0U\ny54Rd+EvqdMp8YDags5wbs/mOsVGZ5g/OmFzudEx5p1eo1MMcOY6w8yKDu64znGuI3QLIcifo/Ev\nKDpFAmh2nkGzg2x2lI0OstlRFp1h0QHmj8HccrNDjCAI8g4wyDvVRmdoQUgYhARhkP91GUSEYYhF\nEWHxGeGijjQMIoIwIAxCwjBY8Z1kJyP0fwX82t1/A2Bme4D3Agr0PmBhRBhGhJXBXjdlZcjyziXv\nuFLSdO4xLTqxLEvI0oQsS8nSBE9S0ixf92JblubLeYeZ5NuzBE8z3BO88XzRyWVZUnR2jW0JXrSF\nLMWLR7Js3nLa7PwaHV++nM3rDLPidY1t+aN53tGZZ1B0gnHRORoZQZbmj0WnmHeSc+vzl8OFXQdh\n8drGDLSySd3mOkYCHJt7bHaMrTpLwwk49u9u5uLL3r2sbewk0M8DfjtvfQL41501R2SFCgIgyM8L\nxHmNUTrgnndYaUKWJUUHmRWPCzvGLJ17TBt/8WXZXIdYvJZGp1p0Xp6m+XKWFB1iNtcxeqNjnbdc\ndHLMX57XGTYfm38p5p0eWd4ZumcLOkia6/nr1g6vW/b/WZf9pKiZ7QR2Apx//vnL/XUiUgZmeTkk\njAhYRbMzllknc96eAV47b31jsW0Bd7/V3cfcfWx0dLSDrxMRkRPpJNB/ClxoZpvNrALsAL7bnWaJ\niMjvasl/6bh7YmYfB75PXlL8prs/0rWWiYjI76Sj0pW7fw/4XpfaIiIiHdB14yIifUKBLiLSJxTo\nIiJ9QoEuItInTuv90M1sEnhqiW8/C3ihi80pA+3z6qB9Xh062ecL3P2kF/Kc1kDvhJmNn8oN3vuJ\n9nl10D6vDqdjn1VyERHpEwp0EZE+UaZAv7XXDegB7fPqoH1eHZZ9n0tTQxcRkRMr0whdREROoBSB\nbmaXm9ljZvZrM7u+1+3pNjN7rZn90MweNbNHzOzaYvt6M7vHzB4vHs/sdVu7zcxCM/u5md1drG82\nsweLY/3t4k6efcPM1pnZnWb2KzPbb2aX9ftxNrNPFP9dP2xmu82s2m/H2cy+aWaHzOzhedtaHlfL\nfa3Y91+a2Zu61Y4VH+jzfrv03cDFwNVmdnFvW9V1CfBJd78YeAvwH4t9vB64z90vBO4r1vvNtcD+\neetfBP7K3f8F8BJwTU9atXy+CvyDu/8e8Ebyfe/b42xm5wG7gDF330J+Z9Yd9N9xvh24fNG2dsf1\n3cCFxb+dwNe71YgVH+jM++1Sd68Bjd8u7Rvu/py7/6xYfpX8/+Tnke/nHcXL7gD+uDctXB5mthH4\nI+C2Yt2AtwN3Fi/pq302s7XAHwDfAHD3mru/TJ8fZ/K7ug6aWQQMAc/RZ8fZ3X8EvLhoc7vj+l7g\nbz33ALDOzM7pRjvKEOitfrv0vB61ZdmZ2SbgUuBBYIO7P1c89TywoUfNWi5fAa4DGr8aPAK87O5J\nsd5vx3ozMAn8TVFmus3Mhunj4+zuzwA3A0+TB/kR4CH6+zg3tDuuy5ZpZQj0VcPM1gB/B/y5u78y\n/znPpyP1zZQkM9sOHHL3h3rdltMoAt4EfN3dLwWOsai80ofH+UzyEelm4FxgmONLE33vdB3XMgT6\nKf12admZWUwe5t9y97uKzQcbf4oVj4d61b5l8FbgSjM7QF5Gezt5fXld8ac59N+xngAm3P3BYv1O\n8oDv5+P8DuBJd5909zpwF/mx7+fj3NDuuC5bppUh0Pv+t0uL2vE3gP3u/pfznvou8KFi+UPA35/u\nti0Xd7/B3Te6+ybyY/qP7v7vgR8Cf1K8rN/2+Xngt2Z2UbFpG/AofXycyUstbzGzoeK/88Y+9+1x\nnqfdcf0u8B+K2S5vAY7MK810xt1X/D/gPcA/AU8AN/a6Pcuwf/+G/M+xXwJ7i3/vIa8p3wc8DtwL\nrO91W5dp//8QuLtY/ufAT4BfA98BBnrdvi7v61ZgvDjW/ws4s9+PM/AXwK+Ah4H/AQz023EGdpOf\nI6iT/yV2TbvjChj5zL0ngH3kM4C60g5dKSoi0ifKUHIREZFToEAXEekTCnQRkT6hQBcR6RMKdBGR\nPqFAFxHpEwp0EZE+oUAXEekT/x8ljo18qKKMowAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}